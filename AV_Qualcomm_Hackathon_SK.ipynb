{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "-HQgjsK1_E4f"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_dir = \"/content\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Open and extract the zip file\n",
        "with zipfile.ZipFile(\"images.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"Contents extracted to {extract_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dr6XEcqFQ2Q",
        "outputId": "5e073158-b288-4550-a8dd-250484ae6977"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents extracted to /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Emergency Dataset Class\n",
        "class EmergencyDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        label = int(self.data_frame.iloc[idx, 1])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "9m3tL88kCFgU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train.csv into train and test sets\n",
        "train_csv = '/content/train.csv'\n",
        "data = pd.read_csv(train_csv)\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the train_data into train and validation sets\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the split data into new CSV files\n",
        "train_data.to_csv('train_split.csv', index=False)\n",
        "val_data.to_csv('val_split.csv', index=False)\n",
        "test_data.to_csv('test_split.csv', index=False)\n",
        "\n",
        "print(\"Data has been split into train_split.csv, val_split.csv, and test_split.csv\")\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL5YYqQfEKmU",
        "outputId": "b710f63d-2326-443b-9638-99edd29d4704"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been split into train_split.csv, val_split.csv, and test_split.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "root_dir = '/content/images'\n",
        "train_dataset = EmergencyDataset(csv_file='train_split.csv', root_dir=root_dir, transform=transform)\n",
        "val_dataset = EmergencyDataset(csv_file='val_split.csv', root_dir=root_dir, transform=transform)\n",
        "test_dataset = EmergencyDataset(csv_file='test_split.csv', root_dir=root_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "K6HagFfrE5-m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# Initially freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the last layer\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Modify the fully connected layer\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(model.fc.in_features, 1),  # Output layer with 1 unit for binary classification\n",
        "    nn.Sigmoid()  # Sigmoid activation for binary classification\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define different learning rates for different layers\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.fc.parameters(), 'lr': 1e-3}\n",
        "])\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss"
      ],
      "metadata": {
        "id": "XF0h48qOFzaG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Reshape labels to match the output shape\n",
        "        labels = labels.view(-1, 1).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for val_inputs, val_labels in val_loader:\n",
        "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "            val_labels = val_labels.view(-1, 1).float()\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss += criterion(val_outputs, val_labels).item()\n",
        "            val_predicted = (val_outputs > 0.5).float()\n",
        "            val_total += val_labels.size(0)\n",
        "            val_correct += (val_predicted == val_labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save the model if validation loss has decreased\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'vehicle_classifier.pth')\n",
        "        print(\"Model saved!\")\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('vehicle_classifier.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYfBRmuPF8DR",
        "outputId": "2c5c940c-d8b5-4323-dc93-baffc923c659"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.0087, Accuracy: 99.62%\n",
            "Validation Loss: 0.1431, Validation Accuracy: 96.59%\n",
            "Model saved!\n",
            "Epoch 2/50, Loss: 0.0105, Accuracy: 99.52%\n",
            "Validation Loss: 0.2032, Validation Accuracy: 93.94%\n",
            "Epoch 3/50, Loss: 0.0118, Accuracy: 99.52%\n",
            "Validation Loss: 0.2988, Validation Accuracy: 92.42%\n",
            "Epoch 4/50, Loss: 0.0172, Accuracy: 99.24%\n",
            "Validation Loss: 0.1276, Validation Accuracy: 95.45%\n",
            "Model saved!\n",
            "Epoch 5/50, Loss: 0.0151, Accuracy: 99.43%\n",
            "Validation Loss: 0.1523, Validation Accuracy: 95.83%\n",
            "Epoch 6/50, Loss: 0.0086, Accuracy: 99.62%\n",
            "Validation Loss: 0.2649, Validation Accuracy: 92.42%\n",
            "Epoch 7/50, Loss: 0.0083, Accuracy: 99.52%\n",
            "Validation Loss: 0.1538, Validation Accuracy: 96.97%\n",
            "Epoch 8/50, Loss: 0.0098, Accuracy: 99.33%\n",
            "Validation Loss: 0.1431, Validation Accuracy: 97.35%\n",
            "Epoch 9/50, Loss: 0.0043, Accuracy: 99.81%\n",
            "Validation Loss: 0.1367, Validation Accuracy: 96.21%\n",
            "Epoch 10/50, Loss: 0.0039, Accuracy: 99.71%\n",
            "Validation Loss: 0.1294, Validation Accuracy: 97.35%\n",
            "Epoch 11/50, Loss: 0.0063, Accuracy: 99.71%\n",
            "Validation Loss: 0.1756, Validation Accuracy: 95.45%\n",
            "Epoch 12/50, Loss: 0.0047, Accuracy: 99.62%\n",
            "Validation Loss: 0.1934, Validation Accuracy: 95.83%\n",
            "Epoch 13/50, Loss: 0.0034, Accuracy: 100.00%\n",
            "Validation Loss: 0.2128, Validation Accuracy: 95.45%\n",
            "Epoch 14/50, Loss: 0.0065, Accuracy: 99.43%\n",
            "Validation Loss: 0.1839, Validation Accuracy: 95.83%\n",
            "Epoch 15/50, Loss: 0.0043, Accuracy: 99.71%\n",
            "Validation Loss: 0.1460, Validation Accuracy: 96.97%\n",
            "Epoch 16/50, Loss: 0.0025, Accuracy: 100.00%\n",
            "Validation Loss: 0.1419, Validation Accuracy: 97.35%\n",
            "Epoch 17/50, Loss: 0.0040, Accuracy: 99.71%\n",
            "Validation Loss: 0.1639, Validation Accuracy: 95.83%\n",
            "Epoch 18/50, Loss: 0.0030, Accuracy: 99.71%\n",
            "Validation Loss: 0.1562, Validation Accuracy: 95.83%\n",
            "Epoch 19/50, Loss: 0.0028, Accuracy: 99.81%\n",
            "Validation Loss: 0.1682, Validation Accuracy: 95.45%\n",
            "Epoch 20/50, Loss: 0.0033, Accuracy: 99.90%\n",
            "Validation Loss: 0.2103, Validation Accuracy: 94.32%\n",
            "Epoch 21/50, Loss: 0.0030, Accuracy: 99.62%\n",
            "Validation Loss: 0.1542, Validation Accuracy: 95.83%\n",
            "Epoch 22/50, Loss: 0.0034, Accuracy: 99.71%\n",
            "Validation Loss: 0.1669, Validation Accuracy: 95.45%\n",
            "Epoch 23/50, Loss: 0.0029, Accuracy: 99.90%\n",
            "Validation Loss: 0.1652, Validation Accuracy: 95.45%\n",
            "Epoch 24/50, Loss: 0.0031, Accuracy: 99.81%\n",
            "Validation Loss: 0.1827, Validation Accuracy: 94.70%\n",
            "Epoch 25/50, Loss: 0.0030, Accuracy: 99.71%\n",
            "Validation Loss: 0.1831, Validation Accuracy: 93.94%\n",
            "Epoch 26/50, Loss: 0.0036, Accuracy: 99.62%\n",
            "Validation Loss: 0.1666, Validation Accuracy: 95.83%\n",
            "Epoch 27/50, Loss: 0.0026, Accuracy: 99.90%\n",
            "Validation Loss: 0.1693, Validation Accuracy: 96.21%\n",
            "Epoch 28/50, Loss: 0.0031, Accuracy: 99.71%\n",
            "Validation Loss: 0.1909, Validation Accuracy: 94.70%\n",
            "Epoch 29/50, Loss: 0.0029, Accuracy: 99.81%\n",
            "Validation Loss: 0.2127, Validation Accuracy: 94.32%\n",
            "Epoch 30/50, Loss: 0.0087, Accuracy: 99.43%\n",
            "Validation Loss: 0.1507, Validation Accuracy: 95.08%\n",
            "Epoch 31/50, Loss: 0.0055, Accuracy: 99.62%\n",
            "Validation Loss: 0.1408, Validation Accuracy: 97.73%\n",
            "Epoch 32/50, Loss: 0.0033, Accuracy: 99.62%\n",
            "Validation Loss: 0.1387, Validation Accuracy: 97.35%\n",
            "Epoch 33/50, Loss: 0.0129, Accuracy: 99.24%\n",
            "Validation Loss: 0.1826, Validation Accuracy: 96.59%\n",
            "Epoch 34/50, Loss: 0.0097, Accuracy: 99.71%\n",
            "Validation Loss: 0.2426, Validation Accuracy: 95.83%\n",
            "Epoch 35/50, Loss: 0.0074, Accuracy: 99.52%\n",
            "Validation Loss: 0.2581, Validation Accuracy: 96.21%\n",
            "Epoch 36/50, Loss: 0.0247, Accuracy: 99.24%\n",
            "Validation Loss: 0.3508, Validation Accuracy: 93.94%\n",
            "Epoch 37/50, Loss: 0.0065, Accuracy: 99.71%\n",
            "Validation Loss: 0.2012, Validation Accuracy: 96.97%\n",
            "Epoch 38/50, Loss: 0.0039, Accuracy: 99.81%\n",
            "Validation Loss: 0.2063, Validation Accuracy: 96.97%\n",
            "Epoch 39/50, Loss: 0.0036, Accuracy: 99.71%\n",
            "Validation Loss: 0.2004, Validation Accuracy: 96.97%\n",
            "Epoch 40/50, Loss: 0.0032, Accuracy: 99.81%\n",
            "Validation Loss: 0.2007, Validation Accuracy: 95.83%\n",
            "Epoch 41/50, Loss: 0.0031, Accuracy: 99.81%\n",
            "Validation Loss: 0.1832, Validation Accuracy: 96.21%\n",
            "Epoch 42/50, Loss: 0.0029, Accuracy: 99.81%\n",
            "Validation Loss: 0.2016, Validation Accuracy: 95.83%\n",
            "Epoch 43/50, Loss: 0.0032, Accuracy: 99.81%\n",
            "Validation Loss: 0.2166, Validation Accuracy: 95.08%\n",
            "Epoch 44/50, Loss: 0.0028, Accuracy: 99.81%\n",
            "Validation Loss: 0.2213, Validation Accuracy: 95.45%\n",
            "Epoch 45/50, Loss: 0.0027, Accuracy: 99.71%\n",
            "Validation Loss: 0.2062, Validation Accuracy: 96.21%\n",
            "Epoch 46/50, Loss: 0.0034, Accuracy: 99.62%\n",
            "Validation Loss: 0.1998, Validation Accuracy: 96.21%\n",
            "Epoch 47/50, Loss: 0.0028, Accuracy: 99.71%\n",
            "Validation Loss: 0.2017, Validation Accuracy: 95.45%\n",
            "Epoch 48/50, Loss: 0.0028, Accuracy: 99.81%\n",
            "Validation Loss: 0.2058, Validation Accuracy: 95.08%\n",
            "Epoch 49/50, Loss: 0.0029, Accuracy: 99.71%\n",
            "Validation Loss: 0.1976, Validation Accuracy: 95.83%\n",
            "Epoch 50/50, Loss: 0.0026, Accuracy: 99.81%\n",
            "Validation Loss: 0.2020, Validation Accuracy: 95.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-68fc0f57b2ca>:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('vehicle_classifier.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset Class for Test Data\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, self.data_frame.iloc[idx, 0]\n",
        "\n",
        "# Load the test data\n",
        "test_csv = '/content/test.csv'\n",
        "test_dataset = TestDataset(csv_file=test_csv, root_dir=root_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Perform inference on the test data and write results to sample_submissions.csv\n",
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, img_names in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        for img_name, pred in zip(img_names, predicted):\n",
        "            results.append([img_name, pred.item()])\n",
        "\n",
        "# Save the results to sample_submissions.csv\n",
        "submission_df = pd.DataFrame(results, columns=['image_names', 'emergency_or_not'])\n",
        "submission_df.to_csv('sample_submissions.csv', index=False)\n",
        "\n",
        "print(\"Results have been written to sample_submissions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbIuaQxjTwQx",
        "outputId": "4e8881fd-0167-44d4-917b-50eea73ccfcc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results have been written to sample_submissions.csv\n"
          ]
        }
      ]
    }
  ]
}