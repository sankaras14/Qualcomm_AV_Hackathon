{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghRBT0REM2eZ"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f1WXXN-NZr_",
        "outputId": "e1424a96-6790-41e9-c580-a17e0fe63246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents extracted to /content/train\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Extract the zip file\n",
        "# Create a directory for extraction\n",
        "extract_dir = \"/content/train\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Open and extract the zip file\n",
        "with zipfile.ZipFile(\"train.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"Contents extracted to {extract_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1ncZcnINVxC",
        "outputId": "74ef6258-0c3f-431d-8be8-2d16a91fd8e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents extracted to /content/test\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Extract the zip file\n",
        "# Create a directory for extraction\n",
        "extract_dir = \"/content/test\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Open and extract the zip file\n",
        "with zipfile.ZipFile(\"test.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"Contents extracted to {extract_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sL0BxK84NxGa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Set device (GPU if available)\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_8V67BD8XiP3"
      },
      "outputs": [],
      "source": [
        "# Define transformations for the training and test datasets\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet mean/std\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "pMW4L_bSXkub"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(root='/content/train/train', transform=train_transforms)\n",
        "test_dataset = datasets.ImageFolder(root='/content/test/test', transform=test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "I-45EdYaXwNz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Split training data into training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_data, val_data = random_split(train_dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "SEIs0bQFX0H7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "N122eajQX3h3"
      },
      "outputs": [],
      "source": [
        "# Define the CNN model architecture (no modular code)\n",
        "conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "fc1 = nn.Linear(128 * 16 * 16, 512)\n",
        "fc2 = nn.Linear(512, 5)  # 5 classes (cycling, dancing, drinking, eating, sitting)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "GJC1UXyXX6wT"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = nn.Sequential(\n",
        "    conv1,\n",
        "    nn.ReLU(),\n",
        "    pool,\n",
        "    conv2,\n",
        "    nn.ReLU(),\n",
        "    pool,\n",
        "    conv3,\n",
        "    nn.ReLU(),\n",
        "    pool,\n",
        "    nn.Flatten(),\n",
        "    fc1,\n",
        "    nn.ReLU(),\n",
        "    fc2,\n",
        "    nn.Softmax()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "jlGhHjuEX9bL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # for classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSeB5TpBYHx2",
        "outputId": "02ad567c-1ebb-4403-aed7-4d8be93d37ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 1.5405, Accuracy: 33.65%\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        #inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track the loss and accuracy\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC6fl00HYbog",
        "outputId": "f059c6ed-8f6d-4f26-f1e2-4284a4f87dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 49.30%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluate the model on the validation set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "val_correct = 0\n",
        "val_total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        #inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        val_total += labels.size(0)\n",
        "        val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "val_accuracy = 100 * val_correct / val_total\n",
        "print(f'Validation Accuracy: {val_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERm6_fktNYjy",
        "outputId": "16d680df-ce36-4afe-d117-74d2daaa74f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 55.08%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Evaluate on the test set\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        #inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * test_correct / test_total\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VMqPyx3yaUu4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "fNQsuym0aOX2",
        "outputId": "d963fc58-ecec-42cb-e76a-94c5bccabca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.77      0.70       126\n",
            "           1       0.62      0.28      0.38       126\n",
            "           2       0.52      0.65      0.58       126\n",
            "           3       0.49      0.72      0.59       126\n",
            "           4       0.54      0.33      0.41       126\n",
            "\n",
            "    accuracy                           0.55       630\n",
            "   macro avg       0.56      0.55      0.53       630\n",
            "weighted avg       0.56      0.55      0.53       630\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 55.08%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on the test set and generate confusion matrix\n",
        "model.eval()\n",
        "test_labels = []\n",
        "test_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        #inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        test_labels.extend(labels.cpu().numpy())  # Collect true labels\n",
        "        test_predictions.extend(predicted.cpu().numpy())  # Collect predicted labels\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, test_predictions)\n",
        "cr=classification_report(test_labels, test_predictions)\n",
        "\n",
        "print(cr)\n",
        "\n",
        "# Plot the confusion matrix using Seaborn heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n",
        "\n",
        "# Print the test accuracy\n",
        "test_accuracy = 100 * sum(np.array(test_labels) == np.array(test_predictions)) / len(test_labels)\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N1wHbaoJbKbX",
        "outputId": "91bc8638-87e5-4053-ad80-f6d64f4dd589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.4532, Accuracy=37.85%\n",
            "Epoch [2/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.1421, Accuracy=55.95%\n",
            "Epoch [3/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.9686, Accuracy=63.10%\n",
            "Epoch [4/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.8091, Accuracy=69.12%\n",
            "Epoch [5/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.6530, Accuracy=74.19%\n",
            "Epoch [6/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.5074, Accuracy=81.41%\n",
            "Epoch [7/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.3770, Accuracy=86.06%\n",
            "Epoch [8/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.2729, Accuracy=90.30%\n",
            "Epoch [9/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.2148, Accuracy=92.82%\n",
            "Epoch [10/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.1760, Accuracy=93.73%\n",
            "Validation Accuracy for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: 62.61%\n",
            "Epoch [1/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.4788, Accuracy=36.24%\n",
            "Epoch [2/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.1810, Accuracy=54.48%\n",
            "Epoch [3/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.9786, Accuracy=62.15%\n",
            "Epoch [4/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.7676, Accuracy=71.53%\n",
            "Epoch [5/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.5841, Accuracy=78.68%\n",
            "Epoch [6/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.3335, Accuracy=88.03%\n",
            "Epoch [7/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.2147, Accuracy=92.19%\n",
            "Epoch [8/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.1309, Accuracy=95.48%\n",
            "Epoch [9/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.1077, Accuracy=96.32%\n",
            "Epoch [10/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.0936, Accuracy=96.81%\n",
            "Validation Accuracy for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: 59.52%\n",
            "Epoch [1/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.5407, Accuracy=28.08%\n",
            "Epoch [2/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.3097, Accuracy=46.88%\n",
            "Epoch [3/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.1083, Accuracy=57.63%\n",
            "Epoch [4/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.9991, Accuracy=61.87%\n",
            "Epoch [5/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.8666, Accuracy=68.80%\n",
            "Epoch [6/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.7513, Accuracy=71.85%\n",
            "Epoch [7/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.6020, Accuracy=77.87%\n",
            "Epoch [8/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.4608, Accuracy=83.19%\n",
            "Epoch [9/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.3554, Accuracy=87.32%\n",
            "Epoch [10/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.3062, Accuracy=89.36%\n",
            "Validation Accuracy for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: 59.66%\n",
            "Epoch [1/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.5724, Accuracy=26.79%\n",
            "Epoch [2/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.2779, Accuracy=48.11%\n",
            "Epoch [3/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.0756, Accuracy=57.95%\n",
            "Epoch [4/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.8731, Accuracy=68.07%\n",
            "Epoch [5/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.6982, Accuracy=74.37%\n",
            "Epoch [6/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.4932, Accuracy=83.12%\n",
            "Epoch [7/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.3280, Accuracy=88.48%\n",
            "Epoch [8/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.2287, Accuracy=92.26%\n",
            "Epoch [9/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.1864, Accuracy=93.52%\n",
            "Epoch [10/10] for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.1608, Accuracy=95.13%\n",
            "Validation Accuracy for lr=0.001, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: 59.38%\n",
            "Epoch [1/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.5132, Accuracy=32.95%\n",
            "Epoch [2/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.2620, Accuracy=50.07%\n",
            "Epoch [3/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.1135, Accuracy=58.05%\n",
            "Epoch [4/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.9787, Accuracy=62.82%\n",
            "Epoch [5/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.8136, Accuracy=70.38%\n",
            "Epoch [6/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.7092, Accuracy=74.12%\n",
            "Epoch [7/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.5392, Accuracy=79.20%\n",
            "Epoch [8/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.4705, Accuracy=83.72%\n",
            "Epoch [9/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.3579, Accuracy=86.73%\n",
            "Epoch [10/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.3289, Accuracy=87.43%\n",
            "Validation Accuracy for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: 62.04%\n",
            "Epoch [1/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.4842, Accuracy=34.49%\n",
            "Epoch [2/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.1924, Accuracy=53.40%\n",
            "Epoch [3/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.9964, Accuracy=61.10%\n",
            "Epoch [4/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.8294, Accuracy=68.91%\n",
            "Epoch [5/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.6231, Accuracy=77.70%\n",
            "Epoch [6/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.4575, Accuracy=83.65%\n",
            "Epoch [7/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.2847, Accuracy=90.48%\n",
            "Epoch [8/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.2447, Accuracy=91.07%\n",
            "Epoch [9/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.1930, Accuracy=93.56%\n",
            "Epoch [10/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.1541, Accuracy=95.45%\n",
            "Validation Accuracy for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: 61.06%\n",
            "Epoch [1/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.5613, Accuracy=27.77%\n",
            "Epoch [2/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.4352, Accuracy=37.78%\n",
            "Epoch [3/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.2884, Accuracy=47.44%\n",
            "Epoch [4/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.1446, Accuracy=54.83%\n",
            "Epoch [5/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=0.9810, Accuracy=63.62%\n",
            "Epoch [6/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=0.8756, Accuracy=68.45%\n",
            "Epoch [7/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=0.7158, Accuracy=74.23%\n",
            "Epoch [8/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=0.6380, Accuracy=77.70%\n",
            "Epoch [9/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=0.5044, Accuracy=82.00%\n",
            "Epoch [10/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=0.4323, Accuracy=84.00%\n",
            "Validation Accuracy for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: 57.70%\n",
            "Epoch [1/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.5871, Accuracy=26.65%\n",
            "Epoch [2/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.4185, Accuracy=40.90%\n",
            "Epoch [3/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.2910, Accuracy=47.72%\n",
            "Epoch [4/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.0815, Accuracy=58.96%\n",
            "Epoch [5/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=0.9188, Accuracy=65.55%\n",
            "Epoch [6/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=0.7694, Accuracy=72.34%\n",
            "Epoch [7/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=0.5629, Accuracy=79.52%\n",
            "Epoch [8/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=0.4309, Accuracy=85.57%\n",
            "Epoch [9/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=0.3272, Accuracy=88.45%\n",
            "Epoch [10/10] for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=0.2919, Accuracy=90.20%\n",
            "Validation Accuracy for lr=0.001, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: 59.24%\n",
            "Epoch [1/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.5049, Accuracy=35.50%\n",
            "Epoch [2/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.2081, Accuracy=52.17%\n",
            "Epoch [3/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.0621, Accuracy=59.21%\n",
            "Epoch [4/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.9084, Accuracy=66.25%\n",
            "Epoch [5/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.7515, Accuracy=71.43%\n",
            "Epoch [6/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.6097, Accuracy=76.79%\n",
            "Epoch [7/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.4770, Accuracy=81.90%\n",
            "Epoch [8/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.3346, Accuracy=87.22%\n",
            "Epoch [9/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.2601, Accuracy=90.83%\n",
            "Epoch [10/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=0.1828, Accuracy=93.94%\n",
            "Validation Accuracy for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=128: 61.76%\n",
            "Epoch [1/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.5246, Accuracy=34.77%\n",
            "Epoch [2/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.1801, Accuracy=53.85%\n",
            "Epoch [3/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.9641, Accuracy=63.69%\n",
            "Epoch [4/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.8530, Accuracy=68.24%\n",
            "Epoch [5/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.6793, Accuracy=75.21%\n",
            "Epoch [6/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.5239, Accuracy=81.41%\n",
            "Epoch [7/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.3623, Accuracy=86.87%\n",
            "Epoch [8/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.2210, Accuracy=92.30%\n",
            "Epoch [9/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.1197, Accuracy=96.01%\n",
            "Epoch [10/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=0.1032, Accuracy=96.64%\n",
            "Validation Accuracy for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=1, hidden_units=256: 62.89%\n",
            "Epoch [1/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.5117, Accuracy=33.44%\n",
            "Epoch [2/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.2503, Accuracy=49.82%\n",
            "Epoch [3/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.0648, Accuracy=59.66%\n",
            "Epoch [4/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.8801, Accuracy=67.30%\n",
            "Epoch [5/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.7625, Accuracy=72.79%\n",
            "Epoch [6/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.6271, Accuracy=77.24%\n",
            "Epoch [7/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.4687, Accuracy=83.16%\n",
            "Epoch [8/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.3471, Accuracy=87.43%\n",
            "Epoch [9/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.2870, Accuracy=89.78%\n",
            "Epoch [10/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=0.2198, Accuracy=92.30%\n",
            "Validation Accuracy for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=128: 61.06%\n",
            "Epoch [1/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.5214, Accuracy=32.91%\n",
            "Epoch [2/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.2069, Accuracy=51.96%\n",
            "Epoch [3/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.9904, Accuracy=62.82%\n",
            "Epoch [4/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.8068, Accuracy=70.20%\n",
            "Epoch [5/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.6516, Accuracy=76.79%\n",
            "Epoch [6/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.4952, Accuracy=82.14%\n",
            "Epoch [7/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.3392, Accuracy=87.32%\n",
            "Epoch [8/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.2341, Accuracy=91.88%\n",
            "Epoch [9/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.1729, Accuracy=93.98%\n",
            "Epoch [10/10] for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=0.1536, Accuracy=95.13%\n",
            "Validation Accuracy for lr=0.001, batch_size=32, dropout=0.3, hidden_layers=2, hidden_units=256: 61.34%\n",
            "Epoch [1/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.5511, Accuracy=30.29%\n",
            "Epoch [2/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.2933, Accuracy=47.44%\n",
            "Epoch [3/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.1641, Accuracy=54.87%\n",
            "Epoch [4/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.0592, Accuracy=59.07%\n",
            "Epoch [5/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.9591, Accuracy=62.99%\n",
            "Epoch [6/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.8479, Accuracy=67.44%\n",
            "Epoch [7/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.7493, Accuracy=72.06%\n",
            "Epoch [8/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.6323, Accuracy=76.61%\n",
            "Epoch [9/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.5343, Accuracy=80.15%\n",
            "Epoch [10/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=0.4798, Accuracy=81.65%\n",
            "Validation Accuracy for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=128: 62.75%\n",
            "Epoch [1/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.5568, Accuracy=31.55%\n",
            "Epoch [2/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.3093, Accuracy=46.46%\n",
            "Epoch [3/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.1926, Accuracy=54.03%\n",
            "Epoch [4/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.0232, Accuracy=60.82%\n",
            "Epoch [5/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.8701, Accuracy=67.33%\n",
            "Epoch [6/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.7365, Accuracy=71.74%\n",
            "Epoch [7/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.5857, Accuracy=78.26%\n",
            "Epoch [8/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.4681, Accuracy=82.95%\n",
            "Epoch [9/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.3537, Accuracy=87.61%\n",
            "Epoch [10/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=0.2734, Accuracy=89.99%\n",
            "Validation Accuracy for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=1, hidden_units=256: 59.66%\n",
            "Epoch [1/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.5717, Accuracy=26.30%\n",
            "Epoch [2/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.4170, Accuracy=39.15%\n",
            "Epoch [3/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.2957, Accuracy=46.71%\n",
            "Epoch [4/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.2160, Accuracy=51.96%\n",
            "Epoch [5/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.0785, Accuracy=60.01%\n",
            "Epoch [6/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=0.9740, Accuracy=63.73%\n",
            "Epoch [7/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=0.8656, Accuracy=68.31%\n",
            "Epoch [8/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=0.7483, Accuracy=73.32%\n",
            "Epoch [9/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=0.6219, Accuracy=77.07%\n",
            "Epoch [10/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=0.5381, Accuracy=80.67%\n",
            "Validation Accuracy for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=128: 62.46%\n",
            "Epoch [1/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.6169, Accuracy=21.01%\n",
            "Epoch [2/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.5599, Accuracy=27.94%\n",
            "Epoch [3/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.4062, Accuracy=41.07%\n",
            "Epoch [4/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.2255, Accuracy=51.44%\n",
            "Epoch [5/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.0492, Accuracy=60.40%\n",
            "Epoch [6/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=0.8504, Accuracy=68.31%\n",
            "Epoch [7/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=0.6843, Accuracy=74.54%\n",
            "Epoch [8/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=0.4914, Accuracy=82.74%\n",
            "Epoch [9/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=0.3970, Accuracy=86.10%\n",
            "Epoch [10/10] for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=0.3048, Accuracy=89.60%\n",
            "Validation Accuracy for lr=0.001, batch_size=32, dropout=0.5, hidden_layers=2, hidden_units=256: 58.82%\n",
            "Epoch [1/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=2.8409, Accuracy=20.59%\n",
            "Epoch [2/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.6113, Accuracy=20.17%\n",
            "Epoch [3/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.6104, Accuracy=19.89%\n",
            "Epoch [4/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.6111, Accuracy=20.17%\n",
            "Epoch [5/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.6111, Accuracy=19.57%\n",
            "Epoch [6/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.6109, Accuracy=20.45%\n",
            "Epoch [7/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.6108, Accuracy=19.96%\n",
            "Epoch [8/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.6112, Accuracy=19.50%\n",
            "Epoch [9/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.6112, Accuracy=18.70%\n",
            "Epoch [10/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: Loss=1.6105, Accuracy=20.17%\n",
            "Validation Accuracy for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=128: 20.17%\n",
            "Epoch [1/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=3.8776, Accuracy=19.40%\n",
            "Epoch [2/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.6110, Accuracy=18.17%\n",
            "Epoch [3/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.6106, Accuracy=19.22%\n",
            "Epoch [4/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.6112, Accuracy=19.68%\n",
            "Epoch [5/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.6110, Accuracy=19.85%\n",
            "Epoch [6/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.6110, Accuracy=19.36%\n",
            "Epoch [7/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.6105, Accuracy=20.55%\n",
            "Epoch [8/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.6108, Accuracy=19.50%\n",
            "Epoch [9/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.6117, Accuracy=20.06%\n",
            "Epoch [10/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: Loss=1.6121, Accuracy=19.36%\n",
            "Validation Accuracy for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=1, hidden_units=256: 18.63%\n",
            "Epoch [1/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=2.5664, Accuracy=19.19%\n",
            "Epoch [2/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.6115, Accuracy=20.24%\n",
            "Epoch [3/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.6109, Accuracy=18.52%\n",
            "Epoch [4/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.6111, Accuracy=19.92%\n",
            "Epoch [5/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.6104, Accuracy=19.33%\n",
            "Epoch [6/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.6113, Accuracy=19.78%\n",
            "Epoch [7/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.6110, Accuracy=19.01%\n",
            "Epoch [8/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.6106, Accuracy=19.89%\n",
            "Epoch [9/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.6111, Accuracy=19.54%\n",
            "Epoch [10/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: Loss=1.6109, Accuracy=20.03%\n",
            "Validation Accuracy for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=128: 18.63%\n",
            "Epoch [1/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=3.5090, Accuracy=18.94%\n",
            "Epoch [2/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.6118, Accuracy=20.59%\n",
            "Epoch [3/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.6118, Accuracy=19.50%\n",
            "Epoch [4/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.6111, Accuracy=19.85%\n",
            "Epoch [5/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.6111, Accuracy=19.36%\n",
            "Epoch [6/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.6109, Accuracy=19.85%\n",
            "Epoch [7/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.6115, Accuracy=19.68%\n",
            "Epoch [8/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.6113, Accuracy=18.28%\n",
            "Epoch [9/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.6107, Accuracy=19.50%\n",
            "Epoch [10/10] for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: Loss=1.6106, Accuracy=19.64%\n",
            "Validation Accuracy for lr=0.01, batch_size=16, dropout=0.3, hidden_layers=2, hidden_units=256: 19.33%\n",
            "Epoch [1/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=2.4199, Accuracy=19.01%\n",
            "Epoch [2/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.6115, Accuracy=19.96%\n",
            "Epoch [3/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.6111, Accuracy=19.85%\n",
            "Epoch [4/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.6108, Accuracy=19.99%\n",
            "Epoch [5/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.6111, Accuracy=19.47%\n",
            "Epoch [6/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.6110, Accuracy=19.96%\n",
            "Epoch [7/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.6110, Accuracy=19.08%\n",
            "Epoch [8/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.6109, Accuracy=18.94%\n",
            "Epoch [9/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.6107, Accuracy=19.64%\n",
            "Epoch [10/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: Loss=1.6113, Accuracy=18.52%\n",
            "Validation Accuracy for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=128: 19.33%\n",
            "Epoch [1/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=2.5645, Accuracy=20.31%\n",
            "Epoch [2/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.6114, Accuracy=20.24%\n",
            "Epoch [3/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.6110, Accuracy=20.13%\n",
            "Epoch [4/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.6110, Accuracy=21.46%\n",
            "Epoch [5/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.6029, Accuracy=23.74%\n",
            "Epoch [6/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.5998, Accuracy=23.25%\n",
            "Epoch [7/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.5927, Accuracy=26.23%\n",
            "Epoch [8/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.5474, Accuracy=30.39%\n",
            "Epoch [9/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.5980, Accuracy=23.56%\n",
            "Epoch [10/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: Loss=1.6114, Accuracy=20.03%\n",
            "Validation Accuracy for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=1, hidden_units=256: 19.33%\n",
            "Epoch [1/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.8960, Accuracy=21.18%\n",
            "Epoch [2/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.6128, Accuracy=19.29%\n",
            "Epoch [3/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.6115, Accuracy=20.17%\n",
            "Epoch [4/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.6104, Accuracy=19.96%\n",
            "Epoch [5/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.6113, Accuracy=19.54%\n",
            "Epoch [6/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.6107, Accuracy=19.15%\n",
            "Epoch [7/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.6123, Accuracy=19.12%\n",
            "Epoch [8/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.6109, Accuracy=19.22%\n",
            "Epoch [9/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.6115, Accuracy=17.30%\n",
            "Epoch [10/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: Loss=1.6113, Accuracy=19.40%\n",
            "Validation Accuracy for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=128: 18.63%\n",
            "Epoch [1/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=2.1509, Accuracy=18.70%\n",
            "Epoch [2/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.6119, Accuracy=19.64%\n",
            "Epoch [3/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.6113, Accuracy=22.02%\n",
            "Epoch [4/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.6117, Accuracy=19.12%\n",
            "Epoch [5/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.6107, Accuracy=20.41%\n",
            "Epoch [6/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.6115, Accuracy=19.19%\n",
            "Epoch [7/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.6100, Accuracy=20.48%\n",
            "Epoch [8/10] for lr=0.01, batch_size=16, dropout=0.5, hidden_layers=2, hidden_units=256: Loss=1.6115, Accuracy=19.43%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-5283f75802be>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m                         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"transparency\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Hyperparameters to tune\n",
        "learning_rates = [0.001, 0.01]\n",
        "batch_sizes = [16, 32]\n",
        "dropout_rates = [0.3, 0.5]\n",
        "hidden_layers = [1, 2,3]  # 1 hidden layer or 2 hidden layers\n",
        "hidden_units = [128, 256]  # Size of each hidden layer\n",
        "num_epochs = 10\n",
        "\n",
        "# Placeholder for the best model and hyperparameters\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "# Grid search loop over hyperparameters\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "        for dropout_rate in dropout_rates:\n",
        "            for num_hidden_layers in hidden_layers:\n",
        "                for hidden_units_per_layer in hidden_units:\n",
        "\n",
        "                    # Recreate data loaders with the current batch size\n",
        "                    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "                    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "                    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "                    # Define the CNN model architecture with the current number of hidden layers and dropout rate\n",
        "                    conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "                    conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "                    conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "                    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "                    fc_layers = []\n",
        "\n",
        "                    # Create hidden layers based on the number of hidden layers\n",
        "                    input_size = 128 * 16 * 16  # Image size after convolution and pooling\n",
        "                    for i in range(num_hidden_layers):\n",
        "                        fc_layers.append(nn.Linear(input_size, hidden_units_per_layer))\n",
        "                        fc_layers.append(nn.ReLU())\n",
        "                        fc_layers.append(nn.Dropout(dropout_rate))\n",
        "                        input_size = hidden_units_per_layer  # Update input size for the next layer\n",
        "\n",
        "                    fc_layers.append(nn.Linear(input_size, 5))  # Output layer for 5 classes (cycling, dancing, drinking, eating, sitting)\n",
        "\n",
        "                    # Combine layers into a model\n",
        "                    model = nn.Sequential(\n",
        "                        conv1,\n",
        "                        nn.ReLU(),\n",
        "                        pool,\n",
        "                        conv2,\n",
        "                        nn.ReLU(),\n",
        "                        pool,\n",
        "                        conv3,\n",
        "                        nn.ReLU(),\n",
        "                        pool,\n",
        "                        nn.Flatten(),\n",
        "                        *fc_layers\n",
        "                    ).to(device)\n",
        "\n",
        "                    # Define the loss function and optimizer for this combination of hyperparameters\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "                    # Train the model\n",
        "                    for epoch in range(num_epochs):\n",
        "                        model.train()\n",
        "                        running_loss = 0.0\n",
        "                        correct = 0\n",
        "                        total = 0\n",
        "\n",
        "                        for inputs, labels in train_loader:\n",
        "                            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                            optimizer.zero_grad()\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, labels)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            running_loss += loss.item()\n",
        "                            _, predicted = torch.max(outputs, 1)\n",
        "                            total += labels.size(0)\n",
        "                            correct += (predicted == labels).sum().item()\n",
        "\n",
        "                        epoch_loss = running_loss / len(train_loader)\n",
        "                        epoch_accuracy = 100 * correct / total\n",
        "                        print(f\"Epoch [{epoch+1}/{num_epochs}] for lr={lr}, batch_size={batch_size}, dropout={dropout_rate}, hidden_layers={num_hidden_layers}, hidden_units={hidden_units_per_layer}: Loss={epoch_loss:.4f}, Accuracy={epoch_accuracy:.2f}%\")\n",
        "\n",
        "                    # Evaluate on the validation set after training\n",
        "                    model.eval()\n",
        "                    val_correct = 0\n",
        "                    val_total = 0\n",
        "                    with torch.no_grad():\n",
        "                        for inputs, labels in val_loader:\n",
        "                            inputs, labels = inputs.to(device), labels.to(device)\n",
        "                            outputs = model(inputs)\n",
        "                            _, predicted = torch.max(outputs, 1)\n",
        "                            val_total += labels.size(0)\n",
        "                            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    val_accuracy = 100 * val_correct / val_total\n",
        "                    print(f\"Validation Accuracy for lr={lr}, batch_size={batch_size}, dropout={dropout_rate}, hidden_layers={num_hidden_layers}, hidden_units={hidden_units_per_layer}: {val_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5wlX2tIpl1-"
      },
      "source": [
        "# **Model Building with RESNET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SxG1Ye1nF1w",
        "outputId": "f5c1a398-9951-4d7d-a684-e4a91970678b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Loss: 1.1738, Accuracy: 55.57%\n",
            "Test Accuracy: 58.41%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# Define the data augmentation transforms and normalization\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet mean/std\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets from directories (assuming images are organized in class-wise directories)\n",
        "train_dataset = datasets.ImageFolder(root='/content/train/train', transform=transform_train)\n",
        "test_dataset = datasets.ImageFolder(root='/content/test/test', transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define a simple CNN model (using ResNet18 as a starting point)\n",
        "model = resnet18(pretrained=True)  # Using pre-trained ResNet18\n",
        "num_ftrs = model.fc.in_features  # The number of input features to the final fully connected layer\n",
        "model.fc = nn.Linear(num_ftrs, 5)  # We have 5 classes, so change the final layer\n",
        "\n",
        "# Move the model to GPU if available\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model = model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "epochs = 1  # Define the number of epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        #inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()  # Set model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_acc = 100 * correct / total\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDUf25kNBS7I",
        "outputId": "b2f9b8cd-5369-49df-e584-b67e1b7d3c85"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1178, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Augmentation**"
      ],
      "metadata": {
        "id": "hzQts1-OHxOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Set device (GPU if available)\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations for the training and test datasets with data augmentation\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(128),  # Random crop and resize to 128x128\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    transforms.RandomRotation(20),      # Random rotation of up to 20 degrees\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Random color jitter\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet mean/std\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(root='path_to_train_data', transform=train_transforms)\n",
        "test_dataset = datasets.ImageFolder(root='path_to_test_data', transform=test_transforms)\n",
        "\n",
        "# Split training data into training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Hyperparameters to tune\n",
        "learning_rates = [0.001, 0.01]\n",
        "batch_sizes = [16, 32]\n",
        "dropout_rates = [0.3, 0.5]\n",
        "hidden_layers = [1, 2]  # 1 hidden layer or 2 hidden layers\n",
        "hidden_units = [128, 256]  # Size of each hidden layer\n",
        "num_epochs = 10\n",
        "\n",
        "# Placeholder for the best model and hyperparameters\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "# Grid search loop over hyperparameters\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "        for dropout_rate in dropout_rates:\n",
        "            for num_hidden_layers in hidden_layers:\n",
        "                for hidden_units_per_layer in hidden_units:\n",
        "\n",
        "                    # Recreate data loaders with the current batch size\n",
        "                    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "                    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "                    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "                    # Define the CNN model architecture with the current number of hidden layers and dropout rate\n",
        "                    conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "                    conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "                    conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "                    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "                    fc_layers = []\n",
        "\n",
        "                    # Create hidden layers based on the number of hidden layers\n",
        "                    input_size = 128 * 16 * 16  # Image size after convolution and pooling\n",
        "                    for i in range(num_hidden_layers):\n",
        "                        fc_layers.append(nn.Linear(input_size, hidden_units_per_layer))\n",
        "                        fc_layers.append(nn.ReLU())\n",
        "                        fc_layers.append(nn.Dropout(dropout_rate))\n",
        "                        input_size = hidden_units_per_layer  # Update input size for the next layer\n",
        "\n",
        "                    fc_layers.append(nn.Linear(input_size, 5))  # Output layer for 5 classes (cycling, dancing, drinking, eating, sitting)\n",
        "\n",
        "                    # Combine layers into a model\n",
        "                    model = nn.Sequential(\n",
        "                        conv1,\n",
        "                        nn.ReLU(),\n",
        "                        pool,\n",
        "                        conv2,\n",
        "                        nn.ReLU(),\n",
        "                        pool,\n",
        "                        conv3,\n",
        "                        nn.ReLU(),\n",
        "                        pool,\n",
        "                        nn.Flatten(),\n",
        "                        *fc_layers\n",
        "                    ).to(device)\n",
        "\n",
        "                    # Define the loss function and optimizer for this combination of hyperparameters\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "                    # Train the model\n",
        "                    for epoch in range(num_epochs):\n",
        "                        model.train()\n",
        "                        running_loss = 0.0\n",
        "                        correct = 0\n",
        "                        total = 0\n",
        "\n",
        "                        for inputs, labels in train_loader:\n",
        "                            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                            optimizer.zero_grad()\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, labels)\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                            running_loss += loss.item()\n",
        "                            _, predicted = torch.max(outputs, 1)\n",
        "                            total += labels.size(0)\n",
        "                            correct += (predicted == labels).sum().item()\n",
        "\n",
        "                        epoch_loss = running_loss / len(train_loader)\n",
        "                        epoch_accuracy = 100 * correct / total\n",
        "                        print(f\"Epoch [{epoch+1}/{num_epochs}] for lr={lr}, batch_size={batch_size}, dropout={dropout_rate}, hidden_layers={num_hidden_layers}, hidden_units={hidden_units_per_layer}: Loss={epoch_loss:.4f}, Accuracy={epoch_accuracy:.2f}%\")\n",
        "\n",
        "                    # Evaluate on the validation set after training\n",
        "                    model.eval()\n",
        "                    val_correct = 0\n",
        "                    val_total = 0\n",
        "                    with torch.no_grad():\n",
        "                        for inputs, labels in val_loader:\n",
        "                            inputs, labels = inputs.to(device), labels.to(device)\n",
        "                            outputs = model(inputs)\n",
        "                            _, predicted = torch.max(outputs, 1)\n",
        "                            val_total += labels.size(0)\n",
        "                            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                    val_accuracy = 100 * val_correct / val_total\n",
        "                    print(f\"Validation Accuracy for lr={lr}, batch_size={batch_size}, dropout={dropout_rate}, hidden_layers={num_hidden_layers}, hidden_units={hidden_units_per_layer}: {val_accuracy:.2f}%\")\n",
        "\n",
        "                    # Check if the current model is the best\n",
        "                    if val_accuracy > best_accuracy:\n",
        "                        best_accuracy = val_accuracy\n",
        "                        best_model = model\n",
        "                        best_params = {'lr': lr, 'batch_size': batch_size, 'dropout_rate': dropout_rate,\n",
        "                                       'num_hidden_layers': num_hidden_layers, 'hidden_units_per_layer': hidden_units_per_layer}\n",
        "\n",
        "# Final evaluation on the test set for the best model\n",
        "model = best_model\n",
        "test_labels = []\n",
        "test_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        test_labels.extend(labels.cpu().numpy())  # Collect true labels\n",
        "        test_predictions.extend(predicted.cpu().numpy())  # Collect predicted labels\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, test_predictions)\n",
        "\n",
        "# Plot the confusion matrix using Seaborn heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n",
        "\n",
        "# Print the best hyperparameters and test accuracy\n",
        "test_accuracy = 100 * sum(np.array(test_labels) == np.array(test_predictions)) / len(test_labels)\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqhnyl_kBTiG",
        "outputId": "6d4cd1e2-a45f-49c1-96de-fdadf9c59eff"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.2976, -3.9306, -1.3261,  6.1125,  0.9774],\n",
              "        [-1.1688, -4.0157, -1.5281,  5.6698,  1.9051],\n",
              "        [ 0.3058, -0.6298, -2.1151,  3.2806,  0.1631],\n",
              "        [-3.4606, -1.8910, -2.2786,  7.4812,  1.9043],\n",
              "        [-0.5415, -1.9025, -2.4029,  5.2355,  0.4986],\n",
              "        [ 2.0239, -0.5392, -3.6341,  0.9585,  1.6941],\n",
              "        [ 1.8715, -3.5300, -1.4561,  1.2266,  2.4974],\n",
              "        [-2.4469, -2.9936, -1.5861,  6.7212,  1.3572],\n",
              "        [-2.7928, -2.9025, -0.5059,  5.4754,  1.4820],\n",
              "        [ 0.0389, -1.3042, -1.8090,  1.0159,  1.5178],\n",
              "        [ 2.0995, -1.5713, -1.4399, -0.1538,  0.8535],\n",
              "        [ 3.8153, -1.3857, -2.7225, -1.1230,  1.8713],\n",
              "        [ 0.0121, -2.1671, -2.1334,  3.1590,  1.8415],\n",
              "        [ 0.6926, -1.9873, -0.8021,  1.6959,  0.9335],\n",
              "        [-1.4957, -1.9689, -1.0461,  3.0491,  1.5829],\n",
              "        [-1.3876, -2.1768, -0.4539,  2.2367,  1.6218],\n",
              "        [-2.2989, -4.6137, -1.2547,  7.6312,  1.5405],\n",
              "        [-0.1217, -1.0609, -1.1240,  1.7728,  0.5388],\n",
              "        [-1.6777, -2.1358, -1.4793,  4.8749,  1.1003],\n",
              "        [-2.0071, -1.4343, -1.0611,  4.7784,  0.8750],\n",
              "        [-1.2662, -2.8908, -0.8733,  4.9155,  0.5787],\n",
              "        [-0.9497, -2.2908, -1.9556,  4.4282,  1.4902]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}