{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "-HQgjsK1_E4f"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract images from the two zip files\n",
        "\n",
        "!git clone https://github.com/Prashant-AV/Qualcomm-DL-Hackathon.git\n",
        "\n",
        "# Function to remove spaces from file names in a given directory\n",
        "def remove_spaces_in_filenames(directory):\n",
        "    for filename in os.listdir(directory):\n",
        "        if ' ' in filename:\n",
        "            new_filename = filename.replace(' ', '_')\n",
        "            os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n",
        "            print(f'Renamed: {filename} -> {new_filename}')\n",
        "\n",
        "# Function to extract images from a zip file and return a list of image file paths\n",
        "def extract_images(zip_file, extract_to):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "        return [os.path.join(extract_to, file) for file in zip_ref.namelist() if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Function to combine images and save them to a new zip file\n",
        "def combine_images_to_zip(image_files, output_zip):\n",
        "    with zipfile.ZipFile(output_zip, 'w') as zipf:\n",
        "        for image_file in image_files:\n",
        "            zipf.write(image_file, os.path.basename(image_file))\n",
        "\n",
        "# Specify the directory containing the files\n",
        "directory = '/content/Qualcomm-DL-Hackathon/train'\n",
        "\n",
        "# Remove spaces from file names\n",
        "remove_spaces_in_filenames(directory)\n",
        "\n",
        "zip_file1 = '/content/Qualcomm-DL-Hackathon/train/images_part-1.zip'\n",
        "zip_file2 = '/content/Qualcomm-DL-Hackathon/train/images_part-2.zip'\n",
        "\n",
        "extract_to1 = 'extracted_zip1'\n",
        "extract_to2 = 'extracted_zip2'\n",
        "\n",
        "image_files1 = extract_images(zip_file1, extract_to1)\n",
        "image_files2 = extract_images(zip_file2, extract_to2)\n",
        "\n",
        "# Combine the images and save them to a new zip file\n",
        "combined_image_files = image_files1 + image_files2\n",
        "output_zip = 'images.zip'\n",
        "combine_images_to_zip(combined_image_files, output_zip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ad4tmwRd4NM",
        "outputId": "20a1c65c-c39f-4c0f-d82f-f78412d01bb5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Qualcomm-DL-Hackathon'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 10 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (10/10), 30.68 MiB | 15.20 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "Renamed: images part-1.zip -> images_part-1.zip\n",
            "Renamed: images part-2.zip -> images_part-2.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract_dir = \"/content/images\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Open and extract the zip file\n",
        "with zipfile.ZipFile(\"images.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"Contents extracted to {extract_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dr6XEcqFQ2Q",
        "outputId": "130e5ed1-0b98-41f5-c874-d2cfe292adfa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents extracted to /content/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Emergency Dataset Class\n",
        "class EmergencyDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        label = int(self.data_frame.iloc[idx, 1])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "9m3tL88kCFgU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train.csv into train and test sets\n",
        "train_csv = '/content/Qualcomm-DL-Hackathon/train/train.csv'\n",
        "data = pd.read_csv(train_csv)\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=50)\n",
        "\n",
        "# Further split the train_data into train and validation sets\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=50)\n",
        "\n",
        "# Save the split data into new CSV files\n",
        "train_data.to_csv('train_split.csv', index=False)\n",
        "val_data.to_csv('val_split.csv', index=False)\n",
        "test_data.to_csv('test_split.csv', index=False)\n",
        "\n",
        "print(\"Data has been split into train_split.csv, val_split.csv, and test_split.csv\")\n",
        "\n",
        "# Define transformations\n",
        "#transform = transforms.Compose([\n",
        "#    transforms.Resize((128, 128)),\n",
        "#    transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define transformations\n",
        "#transform = transforms.Compose([\n",
        "#    transforms.Resize((128, 128)),\n",
        "#    transforms.ToTensor(),\n",
        "#    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL5YYqQfEKmU",
        "outputId": "1b7561bc-0462-4745-d2ee-7976471a46d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been split into train_split.csv, val_split.csv, and test_split.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "root_dir = '/content/images'\n",
        "train_dataset = EmergencyDataset(csv_file='train_split.csv', root_dir=root_dir, transform=transform)\n",
        "val_dataset = EmergencyDataset(csv_file='val_split.csv', root_dir=root_dir, transform=transform)\n",
        "test_dataset = EmergencyDataset(csv_file='test_split.csv', root_dir=root_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "K6HagFfrE5-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparams\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "num_classes = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class EmergencyVehicleClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "      super(EmergencyVehicleClassifier, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "      self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "      self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "      self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True)\n",
        "      self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
        "      self.fc2 = nn.Linear(512, num_classes)  # 2 classes(emergency and non-emergency vehicle)\n",
        "      self.dropout = nn.Dropout(0.5)\n",
        "      #self.relu = nn.ReLU()\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model =  EmergencyVehicleClassifier(num_classes=num_classes).to(device)\n",
        "print (model)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Modify the fully connected layer\n",
        "#model.fc2 = nn.Sequential(\n",
        "#    nn.Dropout(0.3),\n",
        "#    nn.Linear(model.fc2.in_features, 1),  # Output layer with 1 unit for binary classification\n",
        "#    nn.Sigmoid()  # Sigmoid activation for binary classification\n",
        "#)\n",
        "#model = model.to(device)\n"
      ],
      "metadata": {
        "id": "XF0h48qOFzaG",
        "outputId": "b03b7d0f-22b5-488a-fe88-9e1660a6dd62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClassifyEmergencyVehicleOrNot(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "  (fc1): Linear(in_features=32768, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Model training function\n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Reshape labels to match the output shape\n",
        "        #labels = labels.view(-1, 1).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "    epoch_acc = 100 * correct / total\n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
        "    return epoch_loss, epoch_acc\n"
      ],
      "metadata": {
        "id": "EYfBRmuPF8DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ## Model validation function\n",
        " def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            #labels = labels.view(-1, 1).float()\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = running_loss / len(loader)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "-Tma55JvAmzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Actual training the model and save the best model over epochs\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss: .4f}, Train Acc: {train_acc:.2f}% \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc: .2f}%\")\n",
        "\n",
        "    # Save the model if validation loss has decreased\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'emergency_vehicle_classifer.pth')\n",
        "        print(\"Model saved!\")\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('emergency_vehicle_classifier.pth'))"
      ],
      "metadata": {
        "id": "MP4i2Wb-S6qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "#actualtest_csv = '/content/Qualcomm-DL-Hackathon/test.csv'\n",
        "#actualtest_dataset = EmergencyDataset(csv_file=actualtest_csv, root_dir=root_dir, transform=transform)\n",
        "#actualtest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Q1l479S6XG46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    class_correct = [0] * num_classes\n",
        "    class_total = [0] * num_classes\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in loader:\n",
        "        inputs , labels = inputs.to(device) , labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        #_, predicted = torch.max(outputs, 1)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for img_name, pred in zip(inputs, predicted):\n",
        "            results.append([img_name, pred.item()])\n",
        "\n",
        "     # Save the results to sample_submissions.csv\n",
        "    submission_df = pd.DataFrame(results, columns=['image_names', 'emergency_or_not'])\n",
        "    submission_df.to_csv('sample_submissions.csv', index=False)\n",
        "\n",
        "    print(f\"test accuracy : {100 * correct / total:.2f}%\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Un9KXXcSXubn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}