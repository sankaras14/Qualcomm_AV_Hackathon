{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "-HQgjsK1_E4f"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract images from the two zip files\n",
        "\n",
        "#!git clone https://github.com/Prashant-AV/Qualcomm-DL-Hackathon.git\n",
        "\n",
        "# Function to remove spaces from file names in a given directory\n",
        "def remove_spaces_in_filenames(directory):\n",
        "    for filename in os.listdir(directory):\n",
        "        if ' ' in filename:\n",
        "            new_filename = filename.replace(' ', '_')\n",
        "            os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n",
        "            print(f'Renamed: {filename} -> {new_filename}')\n",
        "\n",
        "# Function to extract images from a zip file and return a list of image file paths\n",
        "def extract_images(zip_file, extract_to):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "        return [os.path.join(extract_to, file) for file in zip_ref.namelist() if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Function to combine images and save them to a new zip file\n",
        "def combine_images_to_zip(image_files, output_zip):\n",
        "    with zipfile.ZipFile(output_zip, 'w') as zipf:\n",
        "        for image_file in image_files:\n",
        "            zipf.write(image_file, os.path.basename(image_file))\n",
        "\n",
        "# Specify the directory containing the files\n",
        "directory = '/content/Qualcomm-DL-Hackathon/train'\n",
        "\n",
        "# Remove spaces from file names\n",
        "remove_spaces_in_filenames(directory)\n",
        "\n",
        "zip_file1 = '/content/Qualcomm-DL-Hackathon/train/images_part-1.zip'\n",
        "zip_file2 = '/content/Qualcomm-DL-Hackathon/train/images_part-2.zip'\n",
        "\n",
        "extract_to1 = 'extracted_zip1'\n",
        "extract_to2 = 'extracted_zip2'\n",
        "\n",
        "image_files1 = extract_images(zip_file1, extract_to1)\n",
        "image_files2 = extract_images(zip_file2, extract_to2)\n",
        "\n",
        "# Combine the images and save them to a new zip file\n",
        "combined_image_files = image_files1 + image_files2\n",
        "output_zip = 'images.zip'\n",
        "combine_images_to_zip(combined_image_files, output_zip)"
      ],
      "metadata": {
        "id": "4ad4tmwRd4NM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_dir = \"/content/images\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Open and extract the zip file\n",
        "with zipfile.ZipFile(\"images.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(f\"Contents extracted to {extract_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dr6XEcqFQ2Q",
        "outputId": "5cfe71fd-bdd6-4fe9-eb1b-07d25b72ee12"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents extracted to /content/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Emergency Dataset Class\n",
        "class EmergencyDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        label = int(self.data_frame.iloc[idx, 1])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "9m3tL88kCFgU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train.csv into train and test sets\n",
        "train_csv = '/content/Qualcomm-DL-Hackathon/train/train.csv'\n",
        "data = pd.read_csv(train_csv)\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=50)\n",
        "\n",
        "# Further split the train_data into train and validation sets\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=50)\n",
        "\n",
        "# Save the split data into new CSV files\n",
        "train_data.to_csv('train_split.csv', index=False)\n",
        "val_data.to_csv('val_split.csv', index=False)\n",
        "test_data.to_csv('test_split.csv', index=False)\n",
        "\n",
        "print(\"Data has been split into train_split.csv, val_split.csv, and test_split.csv\")\n",
        "\n",
        "# Define transformations\n",
        "#transform = transforms.Compose([\n",
        "#    transforms.Resize((128, 128)),\n",
        "#    transforms.ToTensor(),\n",
        "#    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#])\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(128),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load Data\n",
        "root_dir = '/content/images'\n",
        "train_dataset = EmergencyDataset(csv_file='train_split.csv', root_dir=root_dir, transform=transform)\n",
        "val_dataset = EmergencyDataset(csv_file='val_split.csv', root_dir=root_dir, transform=transform)\n",
        "test_dataset = EmergencyDataset(csv_file='test_split.csv', root_dir=root_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL5YYqQfEKmU",
        "outputId": "5e4470df-b946-43da-bb58-015963df1178"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been split into train_split.csv, val_split.csv, and test_split.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparams\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 10\n",
        "num_classes = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class EmergencyVehicleClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "      super(EmergencyVehicleClassifier, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "      self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "      self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "      self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=True)\n",
        "      self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
        "      self.fc2 = nn.Linear(512, num_classes)  # 2 classes(emergency and non-emergency vehicle)\n",
        "      self.dropout = nn.Dropout(0.5)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the feature map\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "model =  EmergencyVehicleClassifier(num_classes=num_classes).to(device)\n",
        "print (model)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Learning rate scheduler\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# Modify the fully connected layer\n",
        "#model.fc2 = nn.Sequential(\n",
        "#    nn.Dropout(0.3),\n",
        "#    nn.Linear(model.fc2.in_features, 1),  # Output layer with 1 unit for binary classification\n",
        "#    nn.Sigmoid()  # Sigmoid activation for binary classification\n",
        "#)\n",
        "#model = model.to(device)\n"
      ],
      "metadata": {
        "id": "XF0h48qOFzaG",
        "outputId": "3eb06fdd-9068-4787-d994-860d4eabad5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EmergencyVehicleClassifier(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "  (fc1): Linear(in_features=32768, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (relu): ReLU()\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Model training function\n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    print(\"Inside train\")\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Reshape labels to match the output shape\n",
        "        #labels = labels.view(-1, 1).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        labels_one_hot = torch.nn.functional.one_hot(labels , num_classes).float()\n",
        "        #print(labels_one_hot.shape)\n",
        "        loss = criterion(outputs, labels_one_hot)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        predicted = (outputs[0] > 0.5).float()\n",
        "        #_, predicted = torch.max(outputs, 1)\n",
        "        total += labels_one_hot.size(0)\n",
        "        correct += (predicted == labels_one_hot[0]).sum().item()\n",
        "\n",
        "\n",
        "    epoch_acc = 100 * correct / total\n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
        "    return epoch_loss, epoch_acc\n"
      ],
      "metadata": {
        "id": "EYfBRmuPF8DR"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ## Model validation function\n",
        " def validate(model, loader, criterion, device):\n",
        "    print(\"Inside validate\")\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            labels_one_hot = torch.nn.functional.one_hot(labels , num_classes).float()\n",
        "            val_loss = criterion(outputs, labels_one_hot)\n",
        "            running_loss += val_loss.item()\n",
        "            print(outputs)\n",
        "            predicted = (outputs < 0.5).float()\n",
        "            #_, predicted = torch.max(outputs, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = running_loss / len(loader)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "-Tma55JvAmzr"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Actual training the model and save the best model over epochs\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss: .4f}, Train Acc: {train_acc:.2f}% \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc: .2f}%\")\n",
        "\n",
        "    # Save the model if validation loss has decreased\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'emergency_vehicle_classifer.pth')\n",
        "        print(\"Model saved!\")\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('emergency_vehicle_classifier.pth'))"
      ],
      "metadata": {
        "id": "MP4i2Wb-S6qS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "outputId": "e3045427-c6d5-4af5-9304-0677f519fff6"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside train\n",
            "Epoch 1/10, Loss: 0.6173, Accuracy: 3.90%\n",
            "Inside validate\n",
            "tensor([[0.5517, 0.4738],\n",
            "        [0.6272, 0.4033],\n",
            "        [0.5894, 0.4149],\n",
            "        [0.5953, 0.4389],\n",
            "        [0.6596, 0.3356],\n",
            "        [0.4823, 0.5475],\n",
            "        [0.5938, 0.4462],\n",
            "        [0.0451, 0.9566],\n",
            "        [0.6109, 0.3974],\n",
            "        [0.4050, 0.6270],\n",
            "        [0.5403, 0.4868],\n",
            "        [0.0296, 0.9756],\n",
            "        [0.2353, 0.7611],\n",
            "        [0.6740, 0.3585],\n",
            "        [0.1880, 0.8228],\n",
            "        [0.1609, 0.8456],\n",
            "        [0.7365, 0.2763],\n",
            "        [0.6268, 0.4017],\n",
            "        [0.5743, 0.4416],\n",
            "        [0.4155, 0.6178],\n",
            "        [0.6153, 0.4110],\n",
            "        [0.2745, 0.7468],\n",
            "        [0.5324, 0.5153],\n",
            "        [0.6331, 0.3974],\n",
            "        [0.6073, 0.3933],\n",
            "        [0.6290, 0.3718],\n",
            "        [0.6416, 0.3731],\n",
            "        [0.6030, 0.4186],\n",
            "        [0.6690, 0.3440],\n",
            "        [0.3043, 0.7010],\n",
            "        [0.6192, 0.3906],\n",
            "        [0.4771, 0.5536]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-63c3c6988c3d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     print(f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
            "\u001b[0;32m<ipython-input-115-744a80777c4e>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, loader, criterion, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m            \u001b[0;31m#_, predicted = torch.max(outputs, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m            \u001b[0mval_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m            \u001b[0mval_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m    \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "#actualtest_csv = '/content/Qualcomm-DL-Hackathon/test.csv'\n",
        "#actualtest_dataset = EmergencyDataset(csv_file=actualtest_csv, root_dir=root_dir, transform=transform)\n",
        "#actualtest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Q1l479S6XG46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    class_correct = [0] * num_classes\n",
        "    class_total = [0] * num_classes\n",
        "    results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in loader:\n",
        "        inputs , labels = inputs.to(device) , labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        #_, predicted = torch.max(outputs, 1)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        for img_name, pred in zip(inputs, predicted):\n",
        "            results.append([img_name, pred.item()])\n",
        "\n",
        "     # Save the results to sample_submissions.csv\n",
        "    submission_df = pd.DataFrame(results, columns=['image_names', 'emergency_or_not'])\n",
        "    submission_df.to_csv('sample_submissions.csv', index=False)\n",
        "\n",
        "    print(f\"test accuracy : {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Un9KXXcSXubn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model (model, test_loader, device)"
      ],
      "metadata": {
        "id": "1zKy6akZwJbW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}